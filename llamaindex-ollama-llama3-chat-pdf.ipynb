{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PDF files and split that into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"sample-pdf\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = SentenceSplitter(chunk_size=200, chunk_overlap=50)\n",
    "nodes = text_splitter.get_nodes_from_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 pages\n",
      "18 nodes\n"
     ]
    }
   ],
   "source": [
    "print(len(documents), 'pages')\n",
    "print(len(nodes), 'nodes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Chroma as vector db and load Llama3-8B as embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create client and a new collection\n",
    "chroma_client = chromadb.EphemeralClient()\n",
    "chroma_collection = chroma_client.create_collection(\"percobaan\")\n",
    "\n",
    "# define embedding function\n",
    "ollama_embedding = OllamaEmbedding(\n",
    "    model_name=\"llama3:8b\"\n",
    ")\n",
    "\n",
    "# set up ChromaVectorStore and load in data\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents, storage_context=storage_context, embed_model=ollama_embedding\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex(nodes=nodes, storage_context=storage_context, embed_model=ollama_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create retriever and load Llama3-8B as LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = index.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='522b497f-6519-4179-bb6a-7319b22af303', embedding=None, metadata={'page_label': '1', 'file_name': '12_hl2090.pdf', 'file_path': 'c:\\\\Users\\\\lebnovo\\\\llamaindex\\\\..\\\\local-model\\\\sample-pdf\\\\12_hl2090.pdf', 'file_type': 'application/pdf', 'file_size': 211936, 'creation_date': '2024-07-07', 'last_modified_date': '2024-07-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f7d4f17e-1294-4489-9339-6d6a9d0f8d47', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': '12_hl2090.pdf', 'file_path': 'c:\\\\Users\\\\lebnovo\\\\llamaindex\\\\..\\\\local-model\\\\sample-pdf\\\\12_hl2090.pdf', 'file_type': 'application/pdf', 'file_size': 211936, 'creation_date': '2024-07-07', 'last_modified_date': '2024-07-07'}, hash='13cbcdc00389c03673aa45238ea69847d7da59a2ed137fb68a7ffa6f471a2515'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='4dd496db-b69f-42ea-b3dc-505a59f4b70b', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': '12_hl2090.pdf', 'file_path': 'c:\\\\Users\\\\lebnovo\\\\llamaindex\\\\..\\\\local-model\\\\sample-pdf\\\\12_hl2090.pdf', 'file_type': 'application/pdf', 'file_size': 211936, 'creation_date': '2024-07-07', 'last_modified_date': '2024-07-07'}, hash='7aa0d6960b24e5bb99f1a9f44a95e0194063cab542635b1568948b4dd86f22a3'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='92bc0a97-cef4-4031-ac80-6753f2a98429', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='b2255ae3b6368baa35f979b5276363fdb40ae45c88efdef5b135c5a105ee0a0b')}, text='In this way, the course offers an introduction to economic concepts and ideas through their dramatization in literature, while on a deeper level also exploring the timeless an d universal questions prompted  by this staging.  \\n As part of the course requirements , you will complete one group presentation and one literary \\nessay . In addition, you are expected to actively participate  in class discussion. Through the \\ncreative project “ My Way to Wealth ,” you will also reflect on your own attitude to personal \\nsuccess, drawing on both the course  content and the wisdom  of your peers.', mimetype='text/plain', start_char_idx=1137, end_char_idx=1728, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.0),\n",
       " NodeWithScore(node=TextNode(id_='4dd496db-b69f-42ea-b3dc-505a59f4b70b', embedding=None, metadata={'page_label': '1', 'file_name': '12_hl2090.pdf', 'file_path': 'c:\\\\Users\\\\lebnovo\\\\llamaindex\\\\..\\\\local-model\\\\sample-pdf\\\\12_hl2090.pdf', 'file_type': 'application/pdf', 'file_size': 211936, 'creation_date': '2024-07-07', 'last_modified_date': '2024-07-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f7d4f17e-1294-4489-9339-6d6a9d0f8d47', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': '12_hl2090.pdf', 'file_path': 'c:\\\\Users\\\\lebnovo\\\\llamaindex\\\\..\\\\local-model\\\\sample-pdf\\\\12_hl2090.pdf', 'file_type': 'application/pdf', 'file_size': 211936, 'creation_date': '2024-07-07', 'last_modified_date': '2024-07-07'}, hash='13cbcdc00389c03673aa45238ea69847d7da59a2ed137fb68a7ffa6f471a2515'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='5e4750bb-6e79-48e6-852c-a0d980c6453e', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': '12_hl2090.pdf', 'file_path': 'c:\\\\Users\\\\lebnovo\\\\llamaindex\\\\..\\\\local-model\\\\sample-pdf\\\\12_hl2090.pdf', 'file_type': 'application/pdf', 'file_size': 211936, 'creation_date': '2024-07-07', 'last_modified_date': '2024-07-07'}, hash='d5af52f43448d6c820401bd273350d63f869b2394fbc1960f84c4f29ee634543'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='522b497f-6519-4179-bb6a-7319b22af303', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='af6a23b49a5c1dc92fe661e4fbc568ba85a03b1a6122fbfb3f4f6d43dd48bb9a')}, text='In this course , you will study a series of texts from the English -speaking world dealing with \\nthemes of  money, success, and consumerism . You will learn about the mechanisms  of an urban \\neconomy from John Dos Passos’ s sprawling Manhattan Transfer, deepen your understanding of \\ngreed through Frank N orris’s cautionary tale  McTeague , familiarize yourself with feminist \\neconomics through Agnes Smedley’ s long- neglected classic Daughter of Earth, and analyze the \\ntensions  between old and new money through F. Scott Fitzgerald’s The Great Gatsby  and Kevin \\nKwan’s Crazy Rich Asians.', mimetype='text/plain', start_char_idx=543, end_char_idx=1136, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.0)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.retrieve(\"What's the website that provide NTU Academic Integrity Guidelines?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I'm not aware of any specific website that provides NTU Academic Integrity Guidelines. The given context does not mention anything about a website or academic integrity guidelines from Nanyang Technological University (NTU). It appears to be discussing a course syllabus for an economics-related topic. If you're looking for information on academic integrity, I suggest searching online or checking with your institution's official resources."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = Ollama(model=\"llama3:8b\", request_timeout=60.0)\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "response = query_engine.query(\"What's the website that provide NTU Academic Integrity Guidelines?\")\n",
    "display(Markdown(f\"{response}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify the prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response_synthesizer:text_qa_template': SelectorPromptTemplate(metadata={'prompt_type': <PromptType.QUESTION_ANSWER: 'text_qa'>}, template_vars=['context_str', 'query_str'], kwargs={}, output_parser=None, template_var_mappings={}, function_mappings={}, default_template=PromptTemplate(metadata={'prompt_type': <PromptType.QUESTION_ANSWER: 'text_qa'>}, template_vars=['context_str', 'query_str'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template='Context information is below.\\n---------------------\\n{context_str}\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: {query_str}\\nAnswer: '), conditionals=[(<function is_chat_model at 0x00000218AD817420>, ChatPromptTemplate(metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>}, template_vars=['context_str', 'query_str'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, message_templates=[ChatMessage(role=<MessageRole.SYSTEM: 'system'>, content=\"You are an expert Q&A system that is trusted around the world.\\nAlways answer the query using the provided context information, and not prior knowledge.\\nSome rules to follow:\\n1. Never directly reference the given context in your answer.\\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\", additional_kwargs={}), ChatMessage(role=<MessageRole.USER: 'user'>, content='Context information is below.\\n---------------------\\n{context_str}\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: {query_str}\\nAnswer: ', additional_kwargs={})]))]),\n",
       " 'response_synthesizer:refine_template': SelectorPromptTemplate(metadata={'prompt_type': <PromptType.REFINE: 'refine'>}, template_vars=['query_str', 'existing_answer', 'context_msg'], kwargs={}, output_parser=None, template_var_mappings={}, function_mappings={}, default_template=PromptTemplate(metadata={'prompt_type': <PromptType.REFINE: 'refine'>}, template_vars=['query_str', 'existing_answer', 'context_msg'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template=\"The original query is as follows: {query_str}\\nWe have provided an existing answer: {existing_answer}\\nWe have the opportunity to refine the existing answer (only if needed) with some more context below.\\n------------\\n{context_msg}\\n------------\\nGiven the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\\nRefined Answer: \"), conditionals=[(<function is_chat_model at 0x00000218AD817420>, ChatPromptTemplate(metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>}, template_vars=['context_msg', 'query_str', 'existing_answer'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, message_templates=[ChatMessage(role=<MessageRole.USER: 'user'>, content=\"You are an expert Q&A system that strictly operates in two modes when refining existing answers:\\n1. **Rewrite** an original answer using the new context.\\n2. **Repeat** the original answer if the new context isn't useful.\\nNever reference the original answer or context directly in your answer.\\nWhen in doubt, just repeat the original answer.\\nNew Context: {context_msg}\\nQuery: {query_str}\\nOriginal Answer: {existing_answer}\\nNew Answer: \", additional_kwargs={})]))])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts_dict = query_engine.get_prompts()\n",
    "prompts_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "template = (\n",
    "    \"\"\"\n",
    "    Answer the question based on the context below. If you can't \n",
    "    answer the question, reply \"I don't know\".\n",
    "\n",
    "    Context: {context_str}\n",
    "\n",
    "    Question: {query_str}\n",
    "    \"\"\"\n",
    ")\n",
    "qa_template = PromptTemplate(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_template}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response_synthesizer:text_qa_template': PromptTemplate(metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>}, template_vars=['context_str', 'query_str'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template='\\n    Answer the question based on the context below. If you can\\'t \\n    answer the question, reply \"I don\\'t know\".\\n\\n    Context: {context_str}\\n\\n    Question: {query_str}\\n    '),\n",
       " 'response_synthesizer:refine_template': SelectorPromptTemplate(metadata={'prompt_type': <PromptType.REFINE: 'refine'>}, template_vars=['query_str', 'existing_answer', 'context_msg'], kwargs={}, output_parser=None, template_var_mappings={}, function_mappings={}, default_template=PromptTemplate(metadata={'prompt_type': <PromptType.REFINE: 'refine'>}, template_vars=['query_str', 'existing_answer', 'context_msg'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template=\"The original query is as follows: {query_str}\\nWe have provided an existing answer: {existing_answer}\\nWe have the opportunity to refine the existing answer (only if needed) with some more context below.\\n------------\\n{context_msg}\\n------------\\nGiven the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\\nRefined Answer: \"), conditionals=[(<function is_chat_model at 0x00000218AD817420>, ChatPromptTemplate(metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>}, template_vars=['context_msg', 'query_str', 'existing_answer'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, message_templates=[ChatMessage(role=<MessageRole.USER: 'user'>, content=\"You are an expert Q&A system that strictly operates in two modes when refining existing answers:\\n1. **Rewrite** an original answer using the new context.\\n2. **Repeat** the original answer if the new context isn't useful.\\nNever reference the original answer or context directly in your answer.\\nWhen in doubt, just repeat the original answer.\\nNew Context: {context_msg}\\nQuery: {query_str}\\nOriginal Answer: {existing_answer}\\nNew Answer: \", additional_kwargs={})]))])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts_dict = query_engine.get_prompts()\n",
    "prompts_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask the RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I don't know. The provided context does not mention anything about NTU Academic Integrity Guidelines or a specific website where they can be found."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query(\"What's the website that provide NTU Academic Integrity Guidelines?\")\n",
    "display(Markdown(f\"{response}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the main focus of the course HL 2090 Special Topic in Literature, I: Literature and Economics?\n",
      "Answer: The main focus of the course HL 2090 Special Topic in Literature, I: Literature and Economics is an introduction to economic concepts and ideas through their dramatization in literature.\n",
      "\n",
      "Question: What's the website that provide NTU Academic Integrity Guidelines?\n",
      "Answer: I don't know. The provided context does not mention NTU or any academic integrity guidelines, so I'm unable to answer this question.\n",
      "\n",
      "Question: What if you wish to use the materials for your assignments?\n",
      "Answer: If you wish to use the materials for your assignments, you can find them on the course blog. Materials that are not listed under \"Required Texts\" above can be found on the course blog.\n",
      "\n",
      "Question: Can you list the books mentioned in the course?\n",
      "Answer: I don't know. The text only mentions Aesop's fables and Shakespeare, but it does not specify which exact texts or books will be studied in the course.\n",
      "\n",
      "Question: Can you provide the English Language requirement?\n",
      "Answer: According to the context, the English Language test scores required are PTE Academic, TOEFL, and/or IELTS. The validity of these scores varies by test:\n",
      "\n",
      "* PTE Academic: 2 years\n",
      "* TOEFL: 2 years (institution code is 0786)\n",
      "* IELTS: Not accepted\n",
      "\n",
      "No MUET or other English Language tests are mentioned in the context, so it's unclear if they are required or not.\n",
      "\n",
      "Question: What is the minimum acceptable score for the TOEFL iBT test?\n",
      "Answer: I don't know. The context does not provide information about the minimum acceptable score for the TOEFL iBT test. It only mentions that \"PTE Academic, TOEFL and/or IELTS scores is two years\" and provides the NUS institution code for TOEFL, but it does not specify a minimum score.\n",
      "\n",
      "Question: How long is the validity of the MUET score?\n",
      "Answer: According to the context, the answer is: five years.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What is the main focus of the course HL 2090 Special Topic in Literature, I: Literature and Economics?\",\n",
    "    \"What's the website that provide NTU Academic Integrity Guidelines?\",\n",
    "    \"What if you wish to use the materials for your assignments?\",\n",
    "    \"Can you list the books mentioned in the course?\",\n",
    "    \"Can you provide the English Language requirement?\",\n",
    "    \"What is the minimum acceptable score for the TOEFL iBT test?\",\n",
    "    \"How long is the validity of the MUET score?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    response = query_engine.query(question)\n",
    "    print(f\"Answer: {response}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
